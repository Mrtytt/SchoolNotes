# Markov Karar Süreci (MDP) ve Pekiştirmeli Öğrenme (RL)

## **İçerik**
1. Markov Karar Süreci (MDP)
2. Değer İterasyonu
3. Pekiştirmeli Öğrenme (RL)
4. Q-Öğrenme
5. Belirsizlik Altında Planlama

---

## **1. Markov Karar Süreci (MDP)**
MDP, sonuçların kısmen rastgele, kısmen de bir karar vericinin kontrolü altında olduğu ortamlar için karar verme modellemesi sağlayan matematiksel bir çerçevedir. MDP'nin temel bileşenleri:

- **Durumlar (S):** Sistemin olası konfigürasyonları.
- **Eylemler (A):** Tüm olası eylemler kümesi.
- **Geçiş Olasılıkları (P):** Bir durumdan diğerine bir eylem sonucunda geçiş olasılıkları.
- **Ödüller (R):** Eylemler sonucunda durum geçişlerinden alınan geri bildirimler.

### Sıralı Karar Verme
MDP, kümülatif ödülleri maksimize etmek için bir dizi karar vermeyi ele alır.

### Markov Özelliği
Gelecek durum, geçmiş durumlar yerine yalnızca mevcut durum ve eyleme bağlıdır.

---

## **2. Değer İterasyonu**
Değer iterasyonu, bir MDP'deki durumlar için optimal fayda fonksiyonunu hesaplamak üzere kullanılan yinelemeli bir algoritmadır.

### Adımlar:
1. Tüm durumlar için fayda değerlerini sıfırla başlat.
2. Her durumu **Bellman Denklemi** kullanarak yinelemeli olarak güncelle:
   
   \[ U(s) = R(s) + \gamma \max_{a} \sum_{s'} P(s'|s,a)U(s') \]

   burada:
   - \( U(s) \): Durum \( s \)'nin faydası.
   - \( R(s) \): Durum \( s \) için ödül.
   - \( \gamma \): İndirim faktörü (0 < \( \gamma \) < 1).
   - \( P(s'|s,a) \): Geçiş olasılığı.

3. Fayda değerleri yakınsayana kadar tekrarla.

---

## **3. Pekiştirmeli Öğrenme (RL)**
RL, bir ajanın ortamla etkileşime girerek ödüller veya cezalar alması yoluyla karar verme yeteneği kazandığı bir makine öğrenimi paradigmasıdır.

### Öğrenme Özellikleri:
- Bilinmeyen ortamlar için faydalıdır.
- Ajan, aldığı geri bildirimlere dayalı olarak davranışını adapte eder.
- Gözetimli, gözetimsiz veya pekiştirmeli olabilir.

### RL Türleri:
- **Aktif RL:** Ajan aktif olarak keşfeder ve politikasını geliştirir.
- **Pasif RL:** Ajan sabit bir politikayı değerlendirir.

### RL Örnekleri:
- Robotların kontrolü.
- Video oyunları oynama.
- Sistemlerde öneri optimizasyonu.

---

## **4. Q-Öğrenme**
Q-öğrenme, bir MDP için optimal politikayı modelini bilmeden bulmayı amaçlayan bir dış politika RL algoritmasıdır.

### Temel Kavramlar:
- **Q-Değeri (Q(s, a)):** Durum \( s \)'de eylem \( a \)'yı seçmenin ve ardından optimal politikayı takip etmenin beklenen faydası.

### Algoritma:
1. Tüm durum-eylem çiftleri için Q-değerlerini sıfırla başlat.
2. Her etkileşim için:
   - Bir keşif stratejisi (örneğin, epsilon-greedy) kullanarak bir eylem \( a \) seç.
   - Ödülü \( R \) ve bir sonraki durumu \( s' \) gözlemle.
   - Q-değerini güncelle:

     \[ Q(s,a) \leftarrow Q(s,a) + \alpha \big[ R + \gamma \max_{a'} Q(s',a') - Q(s,a) \big] \]

     burada:
     - \( \alpha \): Öğrenme oranı.
     - \( \gamma \): İndirim faktörü.

3. Yakınsama sağlanana kadar tekrarla.

### Örnek:
- Bilinmeyen ödüllere sahip odalarda bir hedef duruma ulaşmaya çalışan bir robot.

---

## **5. Belirsizlik Altında Planlama**
MDP ve RL, aşağıdaki durumlarla ilgili problemleri ele alır:
- Ortamın sonuçları rastgelelik içerir.
- Eylemler ödüller üzerinde gecikmeli etkilere sahiptir.

### Politika Stratejisi:
Bir politika, stokastik ortamda gezinmek için durumları eylemlere eşleyen bir çözümdür.

---

## **Özet**
1. **MDP:** Belirsizlik altında sıralı karar verme için yapılandırılmış bir çerçeve sunar.
2. **RL:** Ortamla etkileşime girerek optimal politikaları keşfetmeye yönelik bir öğrenme paradigmasıdır.
3. **Değer İterasyonu:** MDP'leri çözmek için yinelemeli güncellemeler kullanan bir hesaplama yöntemidir.
4. **Q-Öğrenme:** Ödülleri veya geçişleri önceden bilmeden politikaları öğrenme algoritmasıdır.

